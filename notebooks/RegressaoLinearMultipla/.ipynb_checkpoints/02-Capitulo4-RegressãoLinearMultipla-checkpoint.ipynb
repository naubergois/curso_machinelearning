{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# <font color='blue'>Capítulo 4 - Regressão Linear Múltipla</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****** Este Jupyter Notebook foi atualizado para a versão 3.6.1. da Linguagem Python em 05/07/2017 ******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando o Dataset Boston Houses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. CRIM: per capita crime rate by town \n",
    "2. ZN: proportion of residential land zoned for lots over 25,000 sq.ft. \n",
    "3. INDUS: proportion of non-residential acres per town \n",
    "4. CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) \n",
    "5. NOX: nitric oxides concentration (parts per 10 million) \n",
    "6. RM: average number of rooms per dwelling \n",
    "7. AGE: proportion of owner-occupied units built prior to 1940 \n",
    "8. DIS: weighted distances to five Boston employment centres \n",
    "9. RAD: index of accessibility to radial highways \n",
    "10. TAX: full-value property-tax rate per 10,000 \n",
    "11. PTRATIO: pupil-teacher ratio by town \n",
    "12. B: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town \n",
    "13. LSTAT: % lower status of the population \n",
    "14. TARGET: Median value of owner-occupied homes in $1000's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/py3k/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gerando o dataset\n",
    "boston = load_boston() \n",
    "dataset = pd.DataFrame(boston.data, columns = boston.feature_names)\n",
    "dataset['target'] = boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gerando número de observações e variáveis\n",
    "observations = len(dataset)\n",
    "variables = dataset.columns[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Coletando x e y\n",
    "X = dataset.iloc[:,:-1]\n",
    "y = dataset['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  \n",
       "0     15.3  396.90   4.98  \n",
       "1     17.8  396.90   9.14  \n",
       "2     17.8  392.83   4.03  \n",
       "3     18.7  394.63   2.94  \n",
       "4     18.7  396.90   5.33  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 24. ,  21.6,  34.7,  33.4,  36.2,  28.7,  22.9,  27.1,  16.5,\n",
       "        18.9,  15. ,  18.9,  21.7,  20.4,  18.2,  19.9,  23.1,  17.5,\n",
       "        20.2,  18.2,  13.6,  19.6,  15.2,  14.5,  15.6,  13.9,  16.6,\n",
       "        14.8,  18.4,  21. ,  12.7,  14.5,  13.2,  13.1,  13.5,  18.9,\n",
       "        20. ,  21. ,  24.7,  30.8,  34.9,  26.6,  25.3,  24.7,  21.2,\n",
       "        19.3,  20. ,  16.6,  14.4,  19.4,  19.7,  20.5,  25. ,  23.4,\n",
       "        18.9,  35.4,  24.7,  31.6,  23.3,  19.6,  18.7,  16. ,  22.2,\n",
       "        25. ,  33. ,  23.5,  19.4,  22. ,  17.4,  20.9,  24.2,  21.7,\n",
       "        22.8,  23.4,  24.1,  21.4,  20. ,  20.8,  21.2,  20.3,  28. ,\n",
       "        23.9,  24.8,  22.9,  23.9,  26.6,  22.5,  22.2,  23.6,  28.7,\n",
       "        22.6,  22. ,  22.9,  25. ,  20.6,  28.4,  21.4,  38.7,  43.8,\n",
       "        33.2,  27.5,  26.5,  18.6,  19.3,  20.1,  19.5,  19.5,  20.4,\n",
       "        19.8,  19.4,  21.7,  22.8,  18.8,  18.7,  18.5,  18.3,  21.2,\n",
       "        19.2,  20.4,  19.3,  22. ,  20.3,  20.5,  17.3,  18.8,  21.4,\n",
       "        15.7,  16.2,  18. ,  14.3,  19.2,  19.6,  23. ,  18.4,  15.6,\n",
       "        18.1,  17.4,  17.1,  13.3,  17.8,  14. ,  14.4,  13.4,  15.6,\n",
       "        11.8,  13.8,  15.6,  14.6,  17.8,  15.4,  21.5,  19.6,  15.3,\n",
       "        19.4,  17. ,  15.6,  13.1,  41.3,  24.3,  23.3,  27. ,  50. ,\n",
       "        50. ,  50. ,  22.7,  25. ,  50. ,  23.8,  23.8,  22.3,  17.4,\n",
       "        19.1,  23.1,  23.6,  22.6,  29.4,  23.2,  24.6,  29.9,  37.2,\n",
       "        39.8,  36.2,  37.9,  32.5,  26.4,  29.6,  50. ,  32. ,  29.8,\n",
       "        34.9,  37. ,  30.5,  36.4,  31.1,  29.1,  50. ,  33.3,  30.3,\n",
       "        34.6,  34.9,  32.9,  24.1,  42.3,  48.5,  50. ,  22.6,  24.4,\n",
       "        22.5,  24.4,  20. ,  21.7,  19.3,  22.4,  28.1,  23.7,  25. ,\n",
       "        23.3,  28.7,  21.5,  23. ,  26.7,  21.7,  27.5,  30.1,  44.8,\n",
       "        50. ,  37.6,  31.6,  46.7,  31.5,  24.3,  31.7,  41.7,  48.3,\n",
       "        29. ,  24. ,  25.1,  31.5,  23.7,  23.3,  22. ,  20.1,  22.2,\n",
       "        23.7,  17.6,  18.5,  24.3,  20.5,  24.5,  26.2,  24.4,  24.8,\n",
       "        29.6,  42.8,  21.9,  20.9,  44. ,  50. ,  36. ,  30.1,  33.8,\n",
       "        43.1,  48.8,  31. ,  36.5,  22.8,  30.7,  50. ,  43.5,  20.7,\n",
       "        21.1,  25.2,  24.4,  35.2,  32.4,  32. ,  33.2,  33.1,  29.1,\n",
       "        35.1,  45.4,  35.4,  46. ,  50. ,  32.2,  22. ,  20.1,  23.2,\n",
       "        22.3,  24.8,  28.5,  37.3,  27.9,  23.9,  21.7,  28.6,  27.1,\n",
       "        20.3,  22.5,  29. ,  24.8,  22. ,  26.4,  33.1,  36.1,  28.4,\n",
       "        33.4,  28.2,  22.8,  20.3,  16.1,  22.1,  19.4,  21.6,  23.8,\n",
       "        16.2,  17.8,  19.8,  23.1,  21. ,  23.8,  23.1,  20.4,  18.5,\n",
       "        25. ,  24.6,  23. ,  22.2,  19.3,  22.6,  19.8,  17.1,  19.4,\n",
       "        22.2,  20.7,  21.1,  19.5,  18.5,  20.6,  19. ,  18.7,  32.7,\n",
       "        16.5,  23.9,  31.2,  17.5,  17.2,  23.1,  24.5,  26.6,  22.9,\n",
       "        24.1,  18.6,  30.1,  18.2,  20.6,  17.8,  21.7,  22.7,  22.6,\n",
       "        25. ,  19.9,  20.8,  16.8,  21.9,  27.5,  21.9,  23.1,  50. ,\n",
       "        50. ,  50. ,  50. ,  50. ,  13.8,  13.8,  15. ,  13.9,  13.3,\n",
       "        13.1,  10.2,  10.4,  10.9,  11.3,  12.3,   8.8,   7.2,  10.5,\n",
       "         7.4,  10.2,  11.5,  15.1,  23.2,   9.7,  13.8,  12.7,  13.1,\n",
       "        12.5,   8.5,   5. ,   6.3,   5.6,   7.2,  12.1,   8.3,   8.5,\n",
       "         5. ,  11.9,  27.9,  17.2,  27.5,  15. ,  17.2,  17.9,  16.3,\n",
       "         7. ,   7.2,   7.5,  10.4,   8.8,   8.4,  16.7,  14.2,  20.8,\n",
       "        13.4,  11.7,   8.3,  10.2,  10.9,  11. ,   9.5,  14.5,  14.1,\n",
       "        16.1,  14.3,  11.7,  13.4,   9.6,   8.7,   8.4,  12.8,  10.5,\n",
       "        17.1,  18.4,  15.4,  10.8,  11.8,  14.9,  12.6,  14.1,  13. ,\n",
       "        13.4,  15.2,  16.1,  17.8,  14.9,  14.1,  12.7,  13.5,  14.9,\n",
       "        20. ,  16.4,  17.7,  19.5,  20.2,  21.4,  19.9,  19. ,  19.1,\n",
       "        19.1,  20.1,  19.9,  19.6,  23.2,  29.8,  13.8,  13.3,  16.7,\n",
       "        12. ,  14.6,  21.4,  23. ,  23.7,  25. ,  21.8,  20.6,  21.2,\n",
       "        19.1,  20.6,  15.2,   7. ,   8.1,  13.6,  20.1,  21.8,  24.5,\n",
       "        23.1,  19.7,  18.3,  21.2,  17.5,  16.8,  22.4,  20.6,  23.9,\n",
       "        22. ,  11.9])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando Múltiplos Atributos com StatsModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xc = sm.add_constant(X)\n",
    "modelo_v1 = sm.OLS(y, Xc)\n",
    "modelo_v2 = modelo_v1.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.741</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.734</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   108.1</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 09 Jul 2017</td> <th>  Prob (F-statistic):</th> <td>6.95e-135</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>22:25:21</td>     <th>  Log-Likelihood:    </th> <td> -1498.8</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   506</td>      <th>  AIC:               </th> <td>   3026.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   492</td>      <th>  BIC:               </th> <td>   3085.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    13</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>        <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>   <td>   36.4911</td> <td>    5.104</td> <td>    7.149</td> <td> 0.000</td> <td>   26.462</td> <td>   46.520</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CRIM</th>    <td>   -0.1072</td> <td>    0.033</td> <td>   -3.276</td> <td> 0.001</td> <td>   -0.171</td> <td>   -0.043</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ZN</th>      <td>    0.0464</td> <td>    0.014</td> <td>    3.380</td> <td> 0.001</td> <td>    0.019</td> <td>    0.073</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>INDUS</th>   <td>    0.0209</td> <td>    0.061</td> <td>    0.339</td> <td> 0.735</td> <td>   -0.100</td> <td>    0.142</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CHAS</th>    <td>    2.6886</td> <td>    0.862</td> <td>    3.120</td> <td> 0.002</td> <td>    0.996</td> <td>    4.381</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>NOX</th>     <td>  -17.7958</td> <td>    3.821</td> <td>   -4.658</td> <td> 0.000</td> <td>  -25.302</td> <td>  -10.289</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RM</th>      <td>    3.8048</td> <td>    0.418</td> <td>    9.102</td> <td> 0.000</td> <td>    2.983</td> <td>    4.626</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AGE</th>     <td>    0.0008</td> <td>    0.013</td> <td>    0.057</td> <td> 0.955</td> <td>   -0.025</td> <td>    0.027</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>DIS</th>     <td>   -1.4758</td> <td>    0.199</td> <td>   -7.398</td> <td> 0.000</td> <td>   -1.868</td> <td>   -1.084</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RAD</th>     <td>    0.3057</td> <td>    0.066</td> <td>    4.608</td> <td> 0.000</td> <td>    0.175</td> <td>    0.436</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TAX</th>     <td>   -0.0123</td> <td>    0.004</td> <td>   -3.278</td> <td> 0.001</td> <td>   -0.020</td> <td>   -0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PTRATIO</th> <td>   -0.9535</td> <td>    0.131</td> <td>   -7.287</td> <td> 0.000</td> <td>   -1.211</td> <td>   -0.696</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>B</th>       <td>    0.0094</td> <td>    0.003</td> <td>    3.500</td> <td> 0.001</td> <td>    0.004</td> <td>    0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LSTAT</th>   <td>   -0.5255</td> <td>    0.051</td> <td>  -10.366</td> <td> 0.000</td> <td>   -0.625</td> <td>   -0.426</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>178.029</td> <th>  Durbin-Watson:     </th> <td>   1.078</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 782.015</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.521</td>  <th>  Prob(JB):          </th> <td>1.54e-170</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 8.276</td>  <th>  Cond. No.          </th> <td>1.51e+04</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.741\n",
       "Model:                            OLS   Adj. R-squared:                  0.734\n",
       "Method:                 Least Squares   F-statistic:                     108.1\n",
       "Date:                Sun, 09 Jul 2017   Prob (F-statistic):          6.95e-135\n",
       "Time:                        22:25:21   Log-Likelihood:                -1498.8\n",
       "No. Observations:                 506   AIC:                             3026.\n",
       "Df Residuals:                     492   BIC:                             3085.\n",
       "Df Model:                          13                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         36.4911      5.104      7.149      0.000      26.462      46.520\n",
       "CRIM          -0.1072      0.033     -3.276      0.001      -0.171      -0.043\n",
       "ZN             0.0464      0.014      3.380      0.001       0.019       0.073\n",
       "INDUS          0.0209      0.061      0.339      0.735      -0.100       0.142\n",
       "CHAS           2.6886      0.862      3.120      0.002       0.996       4.381\n",
       "NOX          -17.7958      3.821     -4.658      0.000     -25.302     -10.289\n",
       "RM             3.8048      0.418      9.102      0.000       2.983       4.626\n",
       "AGE            0.0008      0.013      0.057      0.955      -0.025       0.027\n",
       "DIS           -1.4758      0.199     -7.398      0.000      -1.868      -1.084\n",
       "RAD            0.3057      0.066      4.608      0.000       0.175       0.436\n",
       "TAX           -0.0123      0.004     -3.278      0.001      -0.020      -0.005\n",
       "PTRATIO       -0.9535      0.131     -7.287      0.000      -1.211      -0.696\n",
       "B              0.0094      0.003      3.500      0.001       0.004       0.015\n",
       "LSTAT         -0.5255      0.051    -10.366      0.000      -0.625      -0.426\n",
       "==============================================================================\n",
       "Omnibus:                      178.029   Durbin-Watson:                   1.078\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              782.015\n",
       "Skew:                           1.521   Prob(JB):                    1.54e-170\n",
       "Kurtosis:                       8.276   Cond. No.                     1.51e+04\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.51e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo_v2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matriz de Correlação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             CRIM        ZN     INDUS      CHAS       NOX        RM       AGE  \\\n",
      "CRIM     1.000000 -0.199458  0.404471 -0.055295  0.417521 -0.219940  0.350784   \n",
      "ZN      -0.199458  1.000000 -0.533828 -0.042697 -0.516604  0.311991 -0.569537   \n",
      "INDUS    0.404471 -0.533828  1.000000  0.062938  0.763651 -0.391676  0.644779   \n",
      "CHAS    -0.055295 -0.042697  0.062938  1.000000  0.091203  0.091251  0.086518   \n",
      "NOX      0.417521 -0.516604  0.763651  0.091203  1.000000 -0.302188  0.731470   \n",
      "RM      -0.219940  0.311991 -0.391676  0.091251 -0.302188  1.000000 -0.240265   \n",
      "AGE      0.350784 -0.569537  0.644779  0.086518  0.731470 -0.240265  1.000000   \n",
      "DIS     -0.377904  0.664408 -0.708027 -0.099176 -0.769230  0.205246 -0.747881   \n",
      "RAD      0.622029 -0.311948  0.595129 -0.007368  0.611441 -0.209847  0.456022   \n",
      "TAX      0.579564 -0.314563  0.720760 -0.035587  0.668023 -0.292048  0.506456   \n",
      "PTRATIO  0.288250 -0.391679  0.383248 -0.121515  0.188933 -0.355501  0.261515   \n",
      "B       -0.377365  0.175520 -0.356977  0.048788 -0.380051  0.128069 -0.273534   \n",
      "LSTAT    0.452220 -0.412995  0.603800 -0.053929  0.590879 -0.613808  0.602339   \n",
      "\n",
      "              DIS       RAD       TAX   PTRATIO         B     LSTAT  \n",
      "CRIM    -0.377904  0.622029  0.579564  0.288250 -0.377365  0.452220  \n",
      "ZN       0.664408 -0.311948 -0.314563 -0.391679  0.175520 -0.412995  \n",
      "INDUS   -0.708027  0.595129  0.720760  0.383248 -0.356977  0.603800  \n",
      "CHAS    -0.099176 -0.007368 -0.035587 -0.121515  0.048788 -0.053929  \n",
      "NOX     -0.769230  0.611441  0.668023  0.188933 -0.380051  0.590879  \n",
      "RM       0.205246 -0.209847 -0.292048 -0.355501  0.128069 -0.613808  \n",
      "AGE     -0.747881  0.456022  0.506456  0.261515 -0.273534  0.602339  \n",
      "DIS      1.000000 -0.494588 -0.534432 -0.232471  0.291512 -0.496996  \n",
      "RAD     -0.494588  1.000000  0.910228  0.464741 -0.444413  0.488676  \n",
      "TAX     -0.534432  0.910228  1.000000  0.460853 -0.441808  0.543993  \n",
      "PTRATIO -0.232471  0.464741  0.460853  1.000000 -0.177383  0.374044  \n",
      "B        0.291512 -0.444413 -0.441808 -0.177383  1.000000 -0.366087  \n",
      "LSTAT   -0.496996  0.488676  0.543993  0.374044 -0.366087  1.000000  \n"
     ]
    }
   ],
   "source": [
    "# Gerando a matriz\n",
    "X = dataset.iloc[:,:-1]\n",
    "matriz_corr = X.corr()\n",
    "print (matriz_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Criando um Correlation Plot\n",
    "def visualize_correlation_matrix(data, hurdle = 0.0):\n",
    "    R = np.corrcoef(data, rowvar = 0)\n",
    "    R[np.where(np.abs(R) < hurdle)] = 0.0\n",
    "    heatmap = plt.pcolor(R, cmap = mpl.cm.coolwarm, alpha = 0.8)\n",
    "    heatmap.axes.set_frame_on(False)\n",
    "    heatmap.axes.set_yticks(np.arange(R.shape[0]) + 0.5, minor = False)\n",
    "    heatmap.axes.set_xticks(np.arange(R.shape[1]) + 0.5, minor = False)\n",
    "    heatmap.axes.set_xticklabels(variables, minor = False)\n",
    "    plt.xticks(rotation=90)\n",
    "    heatmap.axes.set_yticklabels(variables, minor = False)\n",
    "    plt.tick_params(axis = 'both', which = 'both', bottom = 'off', top = 'off', left = 'off', right = 'off') \n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEdCAYAAAAIIcBlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116ef56a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizando o Plot\n",
    "visualize_correlation_matrix(X, hurdle = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliando a Multicolinearidade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autovalores (Eigenvalues) e Autovetores (Eigenvectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma forma ainda mais automática de detectar associações multicolineares (e descobrir problemas numéricos em uma inversão de matriz) é usar autovetores. Explicados em termos simples, os autovetores são uma maneira muito inteligente de recombinar a variância entre as variáveis, criando novos recursos acumulando toda a variância compartilhada. Tal recombinação pode ser obtida usando a função NumPy linalg.eig, resultando em um vetor de autovalores (representando a quantidade de variância recombinada para cada nova variável) e autovetores (uma matriz nos dizendo como as novas variáveis se relacionam com as antigas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gerando eigenvalues e eigenvectors\n",
    "corr = np.corrcoef(X, rowvar = 0)\n",
    "eigenvalues, eigenvectors = np.linalg.eig(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois de extrair os autovalores, imprimimos em ordem decrescente e procuramos qualquer elemento cujo valor seja próximo de zero ou pequeno em comparação com os outros. Valores próximos a zero podem representar um problema real para equações normais e outros métodos de otimização baseados na inversão matricial. Valores pequenos representam uma fonte elevada, mas não crítica, de multicolinearidade. Se você detectar qualquer um desses valores baixos, anote a posição no vetor (lembre que os índices em Python começam por zero). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O menor valor está na posição 8. Valor buscar a posição 8 no autovetor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6.12265476  1.43206335  1.24116299  0.85779892  0.83456618  0.65965056\n",
      "  0.53901749  0.39654415  0.06351553  0.27743495  0.16916744  0.18616388\n",
      "  0.22025981]\n"
     ]
    }
   ],
   "source": [
    "print (eigenvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando a posição do índice na lista de autovalores, podemos encontrar o vetor específico nos autovetores que contém as variáveis carregadas, ou seja, o nível de associação com os valores originais. No eigenvector, observamos valores nas posições de índice 2, 8 e 9, que estão realmente em destaque em termos de valor absoluto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.04552843  0.08089873  0.25126664 -0.03590431 -0.04389033 -0.04580522\n",
      "  0.03870705  0.01828389  0.63337285 -0.72024335 -0.02350903  0.00485021\n",
      " -0.02477196]\n"
     ]
    }
   ],
   "source": [
    "print (eigenvectors[:,8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora nós imprimimos os nomes das variáveis para saber quais contribuem mais com seus valores para construir o autovetor. Associamos o vetor de variáveis com o eigenvector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDUS RAD TAX\n"
     ]
    }
   ],
   "source": [
    "print (variables[2], variables[8], variables[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tendo encontrado os culpados da multicolinearidade, o que devemos fazer com essas variáveis? A remoção de algumas delas é geralmente a melhor solução."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradiente Descendente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gerando os dados\n",
    "observations = len(dataset)\n",
    "variables = dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos aplicar Feature Scaling através de Padronização ou Normalização. Normalização aplica escala aos dados com intervalos entre 0 e 1. A Padronização divide a média pelo desvio padrão para obter uma unidade de variância. Vamos usar a Padronização (StandardScaler) pois nesse caso esta técnica ajusta os coeficientes e torna a superfície de erros mais \"tratável\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Aplicando Padronização\n",
    "standardization = StandardScaler()\n",
    "Xst = standardization.fit_transform(X)\n",
    "original_means = standardization.mean_\n",
    "originanal_stds = standardization.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gerando X e Y\n",
    "Xst = np.column_stack((Xst,np.ones(observations)))\n",
    "y  = dataset['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def random_w( p ):\n",
    "    return np.array([np.random.normal() for j in range(p)])\n",
    "\n",
    "def hypothesis(X,w):\n",
    "    return np.dot(X,w)\n",
    "\n",
    "def loss(X,w,y):\n",
    "    return hypothesis(X,w) - y\n",
    "\n",
    "def squared_loss(X,w,y):\n",
    "    return loss(X,w,y)**2\n",
    "\n",
    "def gradient(X,w,y):\n",
    "    gradients = list()\n",
    "    n = float(len( y ))\n",
    "    for j in range(len(w)):\n",
    "        gradients.append(np.sum(loss(X,w,y) * X[:,j]) / n)\n",
    "    return gradients\n",
    "\n",
    "def update(X,w,y, alpha = 0.01):\n",
    "    return [t - alpha*g for t, g in zip(w, gradient(X,w,y))]\n",
    "\n",
    "def optimize(X,y, alpha = 0.01, eta = 10**-12, iterations = 1000):\n",
    "    w = random_w(X.shape[1])\n",
    "    path = list()\n",
    "    for k in range(iterations):\n",
    "        SSL = np.sum(squared_loss(X,w,y))\n",
    "        new_w = update(X,w,y, alpha = alpha)\n",
    "        new_SSL = np.sum(squared_loss(X,new_w,y))\n",
    "        w = new_w\n",
    "        if k>=5 and (new_SSL - SSL <= eta and new_SSL - SSL >= -eta):\n",
    "            path.append(new_SSL)\n",
    "            return w, path\n",
    "        if k % (iterations / 20) == 0:\n",
    "            path.append(new_SSL)\n",
    "    return w, path                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeficientes finais padronizados: -0.9204, 1.0810, 0.1430, 0.6822, -2.0601, 2.6706, 0.0211, -3.1044, 2.6588, -2.0759, -2.0622, 0.8566, -3.7487, 22.5328\n"
     ]
    }
   ],
   "source": [
    "# Imprimindo o resultado                           \n",
    "alpha = 0.01\n",
    "w, path = optimize(Xst, y, alpha, eta = 10**-12, iterations = 20000)\n",
    "print (\"Coeficientes finais padronizados: \" + ', '.join(map(lambda x: \"%0.4f\" % x, w)))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Desfazendo a Padronização\n",
    "unstandardized_betas = w[:-1] / originanal_stds\n",
    "unstandardized_bias  = w[-1]-np.sum((original_means / originanal_stds) * w[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    bias:  36.4911\n",
      "    CRIM:  -0.1072\n",
      "      ZN:   0.0464\n",
      "   INDUS:   0.0209\n",
      "    CHAS:   2.6886\n",
      "     NOX: -17.7958\n",
      "      RM:   3.8048\n",
      "     AGE:   0.0008\n",
      "     DIS:  -1.4758\n",
      "     RAD:   0.3057\n",
      "     TAX:  -0.0123\n",
      " PTRATIO:  -0.9535\n",
      "       B:   0.0094\n",
      "   LSTAT:  -0.5255\n"
     ]
    }
   ],
   "source": [
    "# Imprimindo o resultado\n",
    "print ('%8s: %8.4f' % ('bias', unstandardized_bias))\n",
    "for beta,varname in zip(unstandardized_betas, variables):\n",
    "    print ('%8s: %8.4f' % (varname, beta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importância dos Atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Criando um modelo\n",
    "modelo = linear_model.LinearRegression(normalize = False, fit_intercept = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.796 NOX\n",
      " 3.805 RM\n",
      " 2.689 CHAS\n",
      " 1.476 DIS\n",
      " 0.953 PTRATIO\n",
      " 0.525 LSTAT\n",
      " 0.306 RAD\n",
      " 0.107 CRIM\n",
      " 0.046 ZN\n",
      " 0.021 INDUS\n",
      " 0.012 TAX\n",
      " 0.009 B\n",
      " 0.001 AGE\n"
     ]
    }
   ],
   "source": [
    "modelo.fit(X,y)\n",
    "for coef, var in sorted(zip(map(abs, modelo.coef_), dataset.columns[:-1]), reverse = True):\n",
    "    print (\"%6.3f %s\" % (coef,var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "standardization = StandardScaler()\n",
    "Stand_coef_linear_reg = make_pipeline(standardization, modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3.749 LSTAT\n",
      " 3.104 DIS\n",
      " 2.671 RM\n",
      " 2.659 RAD\n",
      " 2.076 TAX\n",
      " 2.062 PTRATIO\n",
      " 2.060 NOX\n",
      " 1.081 ZN\n",
      " 0.920 CRIM\n",
      " 0.857 B\n",
      " 0.682 CHAS\n",
      " 0.143 INDUS\n",
      " 0.021 AGE\n"
     ]
    }
   ],
   "source": [
    "Stand_coef_linear_reg.fit(X,y)\n",
    "for coef, var in sorted(zip(map(abs, Stand_coef_linear_reg.steps[1][1].coef_), dataset.columns[:-1]), reverse = True):\n",
    "    print (\"%6.3f %s\" % (coef,var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando o R Squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelo = linear_model.LinearRegression(normalize = False, fit_intercept = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def r2_est(X,y):\n",
    "    return r2_score(y, modelo.fit(X,y).predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline R2: 0.741\n"
     ]
    }
   ],
   "source": [
    "print ('Baseline R2: %0.3f' %  r2_est(X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.057 LSTAT\n",
      " 0.044 RM\n",
      " 0.029 DIS\n",
      " 0.028 PTRATIO\n",
      " 0.011 NOX\n",
      " 0.011 RAD\n",
      " 0.006 B\n",
      " 0.006 ZN\n",
      " 0.006 TAX\n",
      " 0.006 CRIM\n",
      " 0.005 CHAS\n",
      " 0.000 INDUS\n",
      " 0.000 AGE\n"
     ]
    }
   ],
   "source": [
    "# Gera o impacto de cada atributo no R2\n",
    "r2_impact = list()\n",
    "for j in range(X.shape[1]):\n",
    "    selection = [i for i in range(X.shape[1]) if i!=j]\n",
    "    r2_impact.append(((r2_est(X,y) - r2_est(X.values[:,selection],y)), dataset.columns[j]))\n",
    "    \n",
    "for imp, varname in sorted(r2_impact, reverse = True):\n",
    "    print ('%6.3f %s' %  (imp, varname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obrigado - Data Science Academy - <a href=http://facebook.com/dsacademy>facebook.com/dsacademybr</a>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3k]",
   "language": "python",
   "name": "conda-env-py3k-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
